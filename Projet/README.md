For the project I decided to fine-tune the GPT2 Medium model, to be able to answer medical multi-choice question (MCQ).

## File provided : 

<ins>Dataset 1</ins> : https://www.kaggle.com/competitions/kaggle-llm-science-exam

<ins>Dataset 2</ins> : https://www.kaggle.com/datasets/thedevastator/medmcqa-medical-mcq-dataset?select=train.csv

The dataset 1 was the original chosen for the project. After some test, I decided to select another one (dataset 2), to focus the model on medical questions.

<ins>Model</ins> :

<ins>Loss plot</ins> : They correspond to the loss obtain after training the model. 
- The first correspond to the training with the first dataset with 2 epochs on 200 questions,
- The second to the training on the second dataset with 5 epochs on 6500 questions,
- The third to the training on the second dataset with 1 epochs on 24000 questions.

<ins>CodeProvided</ins> : Python script provided in the TP. Since the project is similar to the lab7, I used it as a base for the project.

<ins>InstructionDataJson</ins> : Json file containing the answer generated by the model on the train data and the test data, to then obtain a score from a llama model.

<ins>Projet.ipynb</ins> : file containing the treatment of the data, training of the model, and testing of the answer with llama.

<ins>MCQ.py</ins> : python script containing the interface of the project. It's a simple tkinter window on which we write the questions and the possible answers, and the model return which answer seems the most probable.

## Result obtained :

At the end of multiple training and adjustement, I was able to obtain a model with 40% accuracy on the test data. I'm not satisfied with this, but I couldn't do more with the ressources and time at hand.

The model is used in a small python script to have an UI for an easier use.

## Detail of the project :

The idea for the project was to train a model to answer MCQ and be a good tool for helping students with test.

For that I used Google Colab Free, which provide 4 hours of GPU. So I already knew that my training was going to be limited and that I may no obtain the result that I want.

I started by using my lab7 who contained a fine tuning. This garantee a chatgpt2 model working. Then I found my first dataset, which contained 200 questions for a MCQ. My first idea was to simple fine tune the model to be able to answer an MCQ, without having a specific field for the knowledge.

I used the alpaca-style prompt format after extracting the information from the CSV to transform my dataset into one usable by the model for the training.
