{"cells":[{"cell_type":"markdown","id":"51c9672d-8d0c-470d-ac2d-1271f8ec3f14","metadata":{"id":"51c9672d-8d0c-470d-ac2d-1271f8ec3f14"},"source":["# Chapter 4 - Exercises\n","\n","> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n","\n","---"]},{"cell_type":"markdown","id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e","metadata":{"id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e"},"source":["# Exercise 4.1: Parameters in the feed forward versus attention module"]},{"cell_type":"markdown","id":"a8d2157e","metadata":{"id":"a8d2157e"},"source":["**Key Exercise Question: How do the parameter counts differ between the `feed-forward` neural network module and `multi-head attention` mechanism in our transformer architecture?**\n","\n","*Methodological Approach:*\n","The investigation focuses on a systematic computational analysis of parameter allocation across two critical transformer neural network components:\n","\n","1. **Feed-Forward Neural Network Module**\n","   - Characterization: Nonlinear transformation module\n","   - Primary computational function: Introducing network complexity and representational capacity\n","   - Parametric considerations: Linear transformation layers, activation functions\n","\n","2. **Multi-Head Attention Mechanism**\n","   - Characterization: Contextual feature interaction module\n","   - Primary computational function: Capturing inter-token relational dynamics\n","   - Parametric considerations: Projection matrices, attention computation\n","\n","*Analytical Objectives:*\n","- Quantify the exact number of trainable parameters in each architectural component\n","- Comparative assessment of parametric complexity\n","- Understand the relative computational resource allocation\n","\n","*Theoretical Implications:*\n","- Insights into architectural parameter efficiency\n","- Empirical understanding of transformer module design\n","- Potential implications for model optimization and architectural design\n","\n","*Computational Methodology:*\n","1. Enumerate parameters in `feed-forward` module\n","2. Enumerate parameters in `multi-head attention` module\n","3. Perform comparative statistical analysis\n","4. Interpret parametric distribution characteristics\n","\n","*Recommended Investigative Approach:*\n","- Utilize precise computational tracing\n","- Consider layer-specific parameter counting\n","- Account for bias terms and weight matrices"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"DCbcQHp5F_Li","executionInfo":{"status":"ok","timestamp":1737200534997,"user_tz":-60,"elapsed":4536,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"DCbcQHp5F_Li","execution_count":2,"outputs":[]},{"cell_type":"code","source":["class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"IgVJrbqbVIB2","executionInfo":{"status":"ok","timestamp":1737200534998,"user_tz":-60,"elapsed":12,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"IgVJrbqbVIB2","execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"KESkFIUlE0-6","executionInfo":{"status":"ok","timestamp":1737200534998,"user_tz":-60,"elapsed":10,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"KESkFIUlE0-6","execution_count":4,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x"],"metadata":{"id":"P8yjIFxlFxwO","executionInfo":{"status":"ok","timestamp":1737200534998,"user_tz":-60,"elapsed":9,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"P8yjIFxlFxwO","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["In the feed forward module we will only take the embedding dimension and the activation function as parameters. This because the first and last steps of the module is to modify the dimension of the batch given in input. The middle step is to apply the activation function to the batch.\n","\n","In the multihead attention module, we will have more parameters. We have the input and output dimension, the context length, the number of head, the potential dropout and the potential bias. This module objective is to calculate the context vector, over multiple head. This require a lot of operation, for example calculating K,V and Q matrix."],"metadata":{"id":"F21O6q5qGMCa"},"id":"F21O6q5qGMCa"},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"6f-6Rw7xJWZq","executionInfo":{"status":"ok","timestamp":1737200534999,"user_tz":-60,"elapsed":7,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"6f-6Rw7xJWZq","execution_count":6,"outputs":[]},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 1,          # Number of attention heads\n","    \"n_layers\": 1,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","# We simplify the configuration to see more easily the number of parameters\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)"],"metadata":{"id":"CxnZesQ7JX3f","executionInfo":{"status":"ok","timestamp":1737200536039,"user_tz":-60,"elapsed":1046,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"CxnZesQ7JX3f","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Transformer module\n","\n","for name,paramaters in model.trf_blocks.named_parameters():\n","  print(name)\n","  print(paramaters.shape)\n","  print(paramaters.numel())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDFqcG0OJvZa","executionInfo":{"status":"ok","timestamp":1737200536039,"user_tz":-60,"elapsed":12,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"cce4bd0c-f072-4822-9b86-8b753cda70d8"},"id":"hDFqcG0OJvZa","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0.att.W_query.weight\n","torch.Size([768, 768])\n","589824\n","0.att.W_key.weight\n","torch.Size([768, 768])\n","589824\n","0.att.W_value.weight\n","torch.Size([768, 768])\n","589824\n","0.att.out_proj.weight\n","torch.Size([768, 768])\n","589824\n","0.att.out_proj.bias\n","torch.Size([768])\n","768\n","0.ff.layers.0.weight\n","torch.Size([3072, 768])\n","2359296\n","0.ff.layers.0.bias\n","torch.Size([3072])\n","3072\n","0.ff.layers.2.weight\n","torch.Size([768, 3072])\n","2359296\n","0.ff.layers.2.bias\n","torch.Size([768])\n","768\n","0.norm1.scale\n","torch.Size([768])\n","768\n","0.norm1.shift\n","torch.Size([768])\n","768\n","0.norm2.scale\n","torch.Size([768])\n","768\n","0.norm2.shift\n","torch.Size([768])\n","768\n"]}]},{"cell_type":"markdown","source":["At the opposite of how the modules were looking, it turns out the feed forward module result in more trainable parameters than the attention module. If we count for each, for the example above, we have :\n","\n","2360064 Parameters for the attention module.\n","\n","4722432 Parameters for the feedforward module.\n","\n","And the more layers and head we had, the bigger the difference will be."],"metadata":{"id":"jMOgl8McL5nO"},"id":"jMOgl8McL5nO"},{"cell_type":"markdown","id":"0f7b7c7f-0fa1-4d30-ab44-e499edd55b6d","metadata":{"id":"0f7b7c7f-0fa1-4d30-ab44-e499edd55b6d"},"source":["# Exercise 4.2: Initialize larger GPT models"]},{"cell_type":"markdown","id":"310b2e05-3ec8-47fc-afd9-83bf03d4aad8","metadata":{"id":"310b2e05-3ec8-47fc-afd9-83bf03d4aad8"},"source":["- **GPT2-small** (the 124M configuration we already implemented):\n","    - \"emb_dim\" = 768\n","    - \"n_layers\" = 12\n","    - \"n_heads\" = 12\n","\n","- **GPT2-medium:**\n","    - \"emb_dim\" = 1024\n","    - \"n_layers\" = 24\n","    - \"n_heads\" = 16\n","\n","- **GPT2-large:**\n","    - \"emb_dim\" = 1280\n","    - \"n_layers\" = 36\n","    - \"n_heads\" = 20\n","\n","- **GPT2-XL:**\n","    - \"emb_dim\" = 1600\n","    - \"n_layers\" = 48\n","    - \"n_heads\" = 25"]},{"cell_type":"markdown","id":"ceed1fd4","metadata":{"id":"ceed1fd4"},"source":["**Key Exercise Question: Can you systematically scale the GPT-2 model architecture from the small configuration to medium, large, and XL variants by exclusively modifying the configuration parameters?**\n","\n","*Architectural Scaling Challenge:*\n","This exercise explores the methodological expansion of the GPT-2 model across different scales, demonstrating how architectural complexity can be incrementally increased through strategic parameter modifications.\n","\n","*Model Variants to Implement:*\n","1. **GPT-2 Small (Current Implementation)**\n","   - Embedding Dimensions (\"emb_dim\"): 768\n","   - Transformer Blocks (\"n_layers\"): 12\n","   - Multi-Head Attention Heads (\"n_heads\"): 12\n","\n","2. **GPT-2 Medium**\n","   - Embedding Dimensions (\"emb_dim\"): 1,024\n","   - Transformer Blocks (\"n_layers\"): 24\n","   - Multi-Head Attention Heads (\"n_heads\"): 16\n","\n","3. **GPT-2 Large**\n","   - Embedding Dimensions (\"emb_dim\"): 1,280\n","   - Transformer Blocks (\"n_layers\"): 36\n","   - Multi-Head Attention Heads (\"n_heads\"): 20\n","\n","4. **GPT-2 XL**\n","   - Embedding Dimensions (\"emb_dim\"): 1,600\n","   - Transformer Blocks (\"n_layers\"): 48\n","   - Multi-Head Attention Heads (\"n_heads\"): 25\n","\n","*Methodological Constraints:*\n","- Modify only the configuration file\n","- Utilize the existing `GPTModel` class without code alterations\n","- Demonstrate parameter scaling capabilities\n","- Calculate total parameters for each model variant\n","\n","**Bonus Challenge:**\n","**Compute the total number of trainable parameters for each model variant, highlighting the exponential growth in model complexity.**\n","\n"]},{"cell_type":"code","source":["GPT_CONFIG_SMALL = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","GPT_CONFIG_MEDIUM = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 1024,         # Embedding dimension\n","    \"n_heads\": 16,          # Number of attention heads\n","    \"n_layers\": 24,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","GPT_CONFIG_LARGE = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 1280,         # Embedding dimension\n","    \"n_heads\": 20,          # Number of attention heads\n","    \"n_layers\": 36,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}\n","GPT_CONFIG_XL = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 1600,         # Embedding dimension\n","    \"n_heads\": 25,          # Number of attention heads\n","    \"n_layers\": 48,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"snIZEJjRM08C","executionInfo":{"status":"ok","timestamp":1737200536039,"user_tz":-60,"elapsed":8,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"snIZEJjRM08C","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["As long as \"emb_dim\"%\"n_head\" = 0 we can scale the gpt-2 architecture. After that it only depends of the RAM available on the computer."],"metadata":{"id":"UzOpXJYkN8r4"},"id":"UzOpXJYkN8r4"},{"cell_type":"code","source":["modelSmall = GPTModel(GPT_CONFIG_SMALL)\n","#modelMedium = GPTModel(GPT_CONFIG_MEDIUM)\n","#modelLarge = GPTModel(GPT_CONFIG_LARGE)\n","#modelXL = GPTModel(GPT_CONFIG_XL)"],"metadata":{"id":"xzyFROWdNOR8","executionInfo":{"status":"ok","timestamp":1737200537514,"user_tz":-60,"elapsed":1481,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"xzyFROWdNOR8","execution_count":10,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in modelSmall.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GAKtsN8NYQj","executionInfo":{"status":"ok","timestamp":1737200635873,"user_tz":-60,"elapsed":318,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"8210a950-7000-4e8b-f8d5-66227fb252e4"},"id":"0GAKtsN8NYQj","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 163,009,536\n"]}]},{"cell_type":"code","source":["del modelSmall"],"metadata":{"id":"6SUn_B4PO_8w","executionInfo":{"status":"ok","timestamp":1737200767369,"user_tz":-60,"elapsed":195,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"6SUn_B4PO_8w","execution_count":18,"outputs":[]},{"cell_type":"code","source":["modelMedium = GPTModel(GPT_CONFIG_MEDIUM)"],"metadata":{"id":"F0lSvrDUOhxB","executionInfo":{"status":"ok","timestamp":1737200663702,"user_tz":-60,"elapsed":5019,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"F0lSvrDUOhxB","execution_count":12,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in modelMedium.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4j4JsgUVOjRr","executionInfo":{"status":"ok","timestamp":1737200665399,"user_tz":-60,"elapsed":191,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"4e2b0450-10a3-4414-d966-1d49e7f033a4"},"id":"4j4JsgUVOjRr","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 406,212,608\n"]}]},{"cell_type":"code","source":["del modelMedium"],"metadata":{"id":"phQM8KOIO7US","executionInfo":{"status":"ok","timestamp":1737200748182,"user_tz":-60,"elapsed":212,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"phQM8KOIO7US","execution_count":16,"outputs":[]},{"cell_type":"code","source":["modelLarge = GPTModel(GPT_CONFIG_LARGE)"],"metadata":{"id":"btvw97TsOpLz","executionInfo":{"status":"ok","timestamp":1737200696417,"user_tz":-60,"elapsed":7821,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"btvw97TsOpLz","execution_count":14,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in modelLarge.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtWcaOEAOrAg","executionInfo":{"status":"ok","timestamp":1737200698207,"user_tz":-60,"elapsed":229,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"4bb7a52c-9f03-4a71-966e-45198d1d6f25"},"id":"FtWcaOEAOrAg","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 838,220,800\n"]}]},{"cell_type":"code","source":["del modelLarge"],"metadata":{"id":"1Mq1D3vqO9f8","executionInfo":{"status":"ok","timestamp":1737200756388,"user_tz":-60,"elapsed":191,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"1Mq1D3vqO9f8","execution_count":17,"outputs":[]},{"cell_type":"code","source":["modelXL = GPTModel(GPT_CONFIG_XL)"],"metadata":{"id":"9RYQL9J2OxJA","executionInfo":{"status":"ok","timestamp":1737200807626,"user_tz":-60,"elapsed":17419,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"9RYQL9J2OxJA","execution_count":20,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in modelXL.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDGNGyF9Oy-P","executionInfo":{"status":"ok","timestamp":1737200809386,"user_tz":-60,"elapsed":214,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"d4ccdc24-255b-4b28-b851-dd597a16c239"},"id":"EDGNGyF9Oy-P","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 1,637,792,000\n"]}]},{"cell_type":"code","source":["del modelXL"],"metadata":{"id":"trlTxvtYPFQ0","executionInfo":{"status":"ok","timestamp":1737200812044,"user_tz":-60,"elapsed":204,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"trlTxvtYPFQ0","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["We can see here, that for each successive model, the number of parameters keep being multiply by 2, to reach a total of 1.6M for the model XL. At this point google collab can barely stock all of them in the memory."],"metadata":{"id":"xWiJaYYnPQ0x"},"id":"xWiJaYYnPQ0x"},{"cell_type":"markdown","id":"f5f2306e-5dc8-498e-92ee-70ae7ec37ac1","metadata":{"id":"f5f2306e-5dc8-498e-92ee-70ae7ec37ac1"},"source":["# Exercise 4.3: Using separate dropout parameters"]},{"cell_type":"markdown","id":"1a7cc6cd","metadata":{"id":"1a7cc6cd"},"source":["**Key Exercise Question: How can we enhance the dropout configuration of the GPT model by implementing layer-specific dropout rates?**\n","\n","*Architectural Dropout Refinement:*\n","The current implementation employs a uniform dropout rate across multiple model components, which presents an opportunity for more nuanced regularization strategies. This exercise challenges you to develop a more sophisticated approach to dropout implementation within neural network architectures.\n","\n","*Dropout Localization:*\n","Three critical architectural components require distinct dropout configurations:\n","1. Embedding Layer\n","2. Shortcut (Residual) Connections\n","3. Multi-Head Attention Module\n","\n","*Methodological Approach:*\n","You must modify the existing `GPT_CONFIG_124M` configuration to:\n","- Replace the monolithic `drop_rate` parameter\n","- Introduce a hierarchical dropout configuration\n","- Maintain the overall structural integrity of the model architecture\n","\n","*Conceptual Challenge:*\n","The exercise requires a deep understanding of:\n","- Regularization techniques in neural network design\n","- The functional role of dropout in different architectural components\n","- Systematic configuration of model hyperparameters"]},{"cell_type":"code","source":["GPT_CONFIG_DROPOUT = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate_emb\": 0.1,       # Dropout rate for Embedding\n","    \"drop_rate_shortcut\": 0.2,   # Dropout rate for Shortcut\n","    \"drop_rate_attention\": 0.3,    # Dropout rate for Attention\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"],"metadata":{"id":"MCtHdDADQPre","executionInfo":{"status":"ok","timestamp":1737201420143,"user_tz":-60,"elapsed":208,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"MCtHdDADQPre","execution_count":26,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate_attention\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate_shortcut\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x"],"metadata":{"id":"yr1g070QQxA8","executionInfo":{"status":"ok","timestamp":1737201422281,"user_tz":-60,"elapsed":281,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"yr1g070QQxA8","execution_count":27,"outputs":[]},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"],"metadata":{"id":"Dk7jhAZlRBlK","executionInfo":{"status":"ok","timestamp":1737201424024,"user_tz":-60,"elapsed":179,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}}},"id":"Dk7jhAZlRBlK","execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["We modify the configuration file, the TransformerBlock class and the GPTModel class, for each to have a specific dropout parameter, and not a common one."],"metadata":{"id":"zj1hYuJMRkGI"},"id":"zj1hYuJMRkGI"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}