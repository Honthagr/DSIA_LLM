{"cells":[{"cell_type":"markdown","id":"d2a7aa70","metadata":{"id":"d2a7aa70"},"source":["# Chapter 3 - Exercises\n","\n","> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n","\n","---"]},{"cell_type":"markdown","id":"33dfa199-9aee-41d4-a64b-7e3811b9a616","metadata":{"id":"33dfa199-9aee-41d4-a64b-7e3811b9a616"},"source":["# Exercise 3.1"]},{"cell_type":"markdown","id":"5fee2cf5-61c3-4167-81b5-44ea155bbaf2","metadata":{"id":"5fee2cf5-61c3-4167-81b5-44ea155bbaf2"},"source":["Observe that the `nn.Linear` layer in `SelfAttention_v2` employs a distinct weight initialization strategy compared to the `nn.Parameter(torch.rand(d_in, d_out))` method utilized in `SelfAttention_v1`, resulting in divergent computational outputs. To validate the fundamental structural similarities between the two implementations, we propose a weight transfer methodology that will demonstrate the potential for convergence between `SelfAttention_v1` and `SelfAttention_v2`.\n","\n","**Key Exercise Question: Can you transfer the weights from `SelfAttention_v2` to `SelfAttention_v1` such that both implementations produce identical output tensors?**\n","\n","*Specific Challenges:*\n","- Recognize that `nn.Linear` stores its weight matrix in a transposed configuration\n","- Carefully map and transfer weights between the two self-attention implementations\n","- Verify that the transferred weights result in mathematically equivalent computational results\n","\n","The primary objective is to systematically transfer weight matrices from an instantiated `SelfAttention_v2` object to a `SelfAttention_v1` instance, requiring a nuanced understanding of the underlying weight matrix representation.\n","\n","Subsequent research focuses on advancing the self-attention mechanism through two critical architectural enhancements:\n","\n","1. **Causal Masking**: This modification introduces a constraint preventing the attention mechanism from accessing future sequence elements. Such a constraint is particularly pivotal in generative language modeling contexts, where each token's prediction must be conditioned exclusively on preceding contextual information.\n","\n","2. **Multi-Head Attention**: This approach involves partitioning the attention mechanism into parallel computational \"heads.\" Each head operates as a distinct learnable feature extractor, capable of capturing diverse representational characteristics across different subspaces and positional contexts. By enabling simultaneous multi-perspective representation learning, this technique substantially augments the model's capacity to process complex, high-dimensional representations.\n","\n","These architectural refinements collectively contribute to more sophisticated and contextually aware neural network architectures, particularly in sequence modeling domains."]},{"cell_type":"code","source":["import torch\n","\n","inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")\n","\n","d_in = inputs.shape[1] # the input embedding size, d=3\n","d_out = 2 # the output embedding size, d=2"],"metadata":{"id":"owy5fM3huqsE"},"id":"owy5fM3huqsE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SelfAttention_v1(nn.Module):\n","\n","    def __init__(self, d_in, d_out):\n","        super().__init__()\n","        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n","        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n","        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n","\n","    def setWeight(self, W_query, W_key, W_value):\n","        self.W_query.data = W_query\n","        self.W_key.data   = W_key\n","        self.W_value.data = W_value\n","\n","    def forward(self, x):\n","        keys = x @ self.W_key\n","        queries = x @ self.W_query\n","        values = x @ self.W_value\n","\n","        attn_scores = queries @ keys.T # omega\n","        attn_weights = torch.softmax(\n","            attn_scores / keys.shape[-1]**0.5, dim=-1\n","        )\n","\n","        context_vec = attn_weights @ values\n","        return context_vec"],"metadata":{"id":"xEvjuzTAqkY7"},"id":"xEvjuzTAqkY7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttention_v2(nn.Module):\n","\n","    def __init__(self, d_in, d_out, qkv_bias=False):\n","        super().__init__()\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","    def getWeight(self):\n","        return torch.stack([\n","            self.W_query.weight.T,\n","            self.W_key.weight.T,\n","            self.W_value.weight.T\n","        ])\n","\n","    def forward(self, x):\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        attn_scores = queries @ keys.T\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","\n","        context_vec = attn_weights @ values\n","        return context_vec"],"metadata":{"id":"w-6zKa23uxei"},"id":"w-6zKa23uxei","execution_count":null,"outputs":[]},{"cell_type":"code","source":["sa_v2 = SelfAttention_v2(d_in, d_out)\n","WQ, WK, WV = sa_v2.getWeight()\n","\n","sa_v1 = SelfAttention_v1(d_in, d_out)\n","sa_v1.setWeight(WQ, WK, WV)\n","\n","print(sa_v2(inputs))\n","print(sa_v1(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJzlcM_au3CU","executionInfo":{"status":"ok","timestamp":1737058726968,"user_tz":-60,"elapsed":319,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"1a084cdd-109b-4fbb-83e1-5451be16d593"},"id":"EJzlcM_au3CU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5322, 0.2491],\n","        [0.5316, 0.2488],\n","        [0.5316, 0.2488],\n","        [0.5340, 0.2501],\n","        [0.5331, 0.2497],\n","        [0.5337, 0.2499]], grad_fn=<MmBackward0>)\n","tensor([[0.5322, 0.2491],\n","        [0.5316, 0.2488],\n","        [0.5316, 0.2488],\n","        [0.5340, 0.2501],\n","        [0.5331, 0.2497],\n","        [0.5337, 0.2499]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Yes you can transfer the weight of SelfAttention_V2 to SelfAttention_V1 and get the same output.\n"],"metadata":{"id":"8_WTRakf0Atv"},"id":"8_WTRakf0Atv"},{"cell_type":"markdown","id":"33543edb-46b5-4b01-8704-f7f101230544","metadata":{"id":"33543edb-46b5-4b01-8704-f7f101230544"},"source":["# Exercise 3.2"]},{"cell_type":"markdown","id":"18e748ef-3106-4e11-a781-b230b74a0cef","metadata":{"id":"18e748ef-3106-4e11-a781-b230b74a0cef"},"source":["**Key Exercise Question: How can you modify the input arguments to the `MultiHeadAttentionWrapper(num_heads=2)` to transform the output context vectors from four-dimensional to two-dimensional while maintaining the `num_heads=2` configuration?**\n","\n","*Specific Challenges:*\n","- Identify the input parameter that controls the dimensionality of output context vectors\n","- Understand the relationship between input arguments and tensor shape\n","- Achieve dimensionality reduction without modifying the core `MultiHeadAttentionWrapper` class implementation\n","\n","*Architectural Context:*\n","Up to this point, we have developed a `MultiHeadAttentionWrapper` that integrates multiple single-head attention modules through sequential processing, implemented via the comprehension `[head(x) for head in self.heads]` in the forward method. This current implementation represents a foundational approach to multi-head attention mechanisms.\n","\n","*Potential Optimization Strategies:*\n","1. **Sequential Processing Limitation**: The current implementation processes attention heads sequentially, which may introduce computational inefficiencies.\n","\n","2. **Parallel Processing Approach**: An advanced optimization involves simultaneous computation of attention head outputs through efficient matrix multiplication techniques. This parallel processing strategy can potentially enhance computational performance and reduce computational overhead.\n","\n","*Theoretical Implications:*\n","The ability to dynamically adjust output dimensionality while maintaining the multi-head attention structure highlights the flexibility of modern neural network architectural designs. Such manipulations are crucial in adapting attention mechanisms to diverse computational requirements across different machine learning domains.\n","\n","*Practical Recommendation:*\n","Carefully examine the input arguments of the `MultiHeadAttentionWrapper` and consider how specific parameters might influence the output tensor's dimensionality. The solution likely involves a subtle adjustment that does not require restructuring the core implementation."]},{"cell_type":"code","source":["batch = torch.stack((inputs, inputs), dim=0)\n","\n","class CausalAttention(nn.Module):\n","\n","    def __init__(self, d_in, d_out, context_length,\n","                 dropout, qkv_bias=False):\n","        super().__init__()\n","        self.d_out = d_out\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.dropout = nn.Dropout(dropout) # New\n","        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape # New batch dimension b\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n","        attn_scores.masked_fill_(  # New, _ ops are in-place\n","            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n","        attn_weights = torch.softmax(\n","            attn_scores / keys.shape[-1]**0.5, dim=-1\n","        )\n","        attn_weights = self.dropout(attn_weights) # New\n","\n","        context_vec = attn_weights @ values\n","        return context_vec"],"metadata":{"id":"gg1v3VLgxic3"},"id":"gg1v3VLgxic3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttentionWrapper(nn.Module):\n","\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        self.heads = nn.ModuleList(\n","            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n","             for _ in range(num_heads)]\n","        )\n","\n","    def forward(self, x):\n","        return torch.cat([head(x) for head in self.heads], dim=-1)"],"metadata":{"id":"LySqQDoXxTJi"},"id":"LySqQDoXxTJi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["context_length = batch.shape[1] # This is the number of tokens\n","d_in, d_out = 3, 1\n","mha = MultiHeadAttentionWrapper(\n","    d_in, d_out, context_length, 0.0, num_heads=2\n",")\n","\n","context_vecs = mha(batch)\n","\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opqujYyIxckd","executionInfo":{"status":"ok","timestamp":1737059027742,"user_tz":-60,"elapsed":364,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"7cc48de0-ae89-4be9-9a0b-44a7c34c58b1"},"id":"opqujYyIxckd","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.4267, -0.2793],\n","         [-0.3547, -0.0639],\n","         [-0.3302,  0.0042],\n","         [-0.2801,  0.0440],\n","         [-0.2459,  0.0209],\n","         [-0.2370,  0.0650]],\n","\n","        [[-0.4267, -0.2793],\n","         [-0.3547, -0.0639],\n","         [-0.3302,  0.0042],\n","         [-0.2801,  0.0440],\n","         [-0.2459,  0.0209],\n","         [-0.2370,  0.0650]]], grad_fn=<CatBackward0>)\n","context_vecs.shape: torch.Size([2, 6, 2])\n"]}]},{"cell_type":"markdown","source":["Since having 2 heads double the dimension of the output, you can divide the d_out by two to get a dimensional output of 2."],"metadata":{"id":"5uLeIA170JZM"},"id":"5uLeIA170JZM"},{"cell_type":"markdown","id":"92bdabcb-06cf-4576-b810-d883bbd313ba","metadata":{"id":"92bdabcb-06cf-4576-b810-d883bbd313ba"},"source":["# Exercise 3.3"]},{"cell_type":"markdown","id":"78fdab01","metadata":{"vscode":{"languageId":"plaintext"},"id":"78fdab01"},"source":["**Key Exercise Question: Can you configure a `MultiHeadAttention` module that precisely replicates the architectural specifications of the smallest GPT-2 model?**\n","\n","*Specific Model Specifications:*\n","- Number of Attention Heads: 12\n","- Input/Output Embedding Dimensions: 768\n","- Context Length: 1,024 tokens\n","\n","*Architectural Parameters:*\n","- `num_heads`: 12\n","- `d_model`: 768\n","- `context_length`: 1,024\n","\n","*Theoretical Considerations:*\n","The proposed configuration mirrors the smallest variant of the GPT-2 model, which represents a fundamental architecture in transformer-based language models. By precisely replicating these specifications, we can explore the intricate design choices that contribute to the model's effectiveness in natural language processing tasks.\n","\n","*Key Implementation Details:*\n","- Ensuring 12 parallel attention heads allows for multi-perspective feature representation\n","- The 768-dimensional embedding space provides a rich, high-dimensional representation of linguistic context\n","- The 1,024 token context length enables comprehensive sequence processing\n","\n","*Practical Recommendation:*\n","Carefully construct the `MultiHeadAttention` initialization to match these exact specifications, paying close attention to the dimensionality and number of heads to accurately reproduce the smallest GPT-2 model's architectural characteristics."]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"qVWea8XJyhFi"},"id":"qVWea8XJyhFi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mha = MultiHeadAttention(d_in=768, d_out=768, context_length=1024, dropout=0.0, num_heads=12)"],"metadata":{"id":"mTLHfV_kyjpR"},"id":"mTLHfV_kyjpR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Yes you can configure MultiHeadAttention() to get the same architectures as GPT2"],"metadata":{"id":"sW8c2TLu0YmR"},"id":"sW8c2TLu0YmR"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}