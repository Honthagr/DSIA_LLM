{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {
    "id": "ab88d307-61ba-45ef-89bc-e3569443dfca"
   },
   "source": [
    "# Chapter 2 - Lab 1a - Exercise\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {
    "id": "6f678e62-7bcb-4405-86ae-dce94f494303"
   },
   "source": [
    "# Exercise 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fde9a6",
   "metadata": {
    "id": "46fde9a6"
   },
   "source": [
    "### Exploring Byte Pair Encoding (BPE) Tokenization with Unknown Words\n",
    "\n",
    "In this exercise, you will explore how the Byte Pair Encoding (BPE) tokenizer from the `tiktoken` library processes unknown words. BPE is a subword tokenization technique that constructs its vocabulary by iteratively merging frequent sequences of characters or subwords. This approach allows BPE tokenizers to handle previously unseen words by decomposing them into smaller, known subunits.\n",
    "\n",
    "### Objective\n",
    "Answer the following questions based on your experimentation with the tokenizer:\n",
    "\n",
    "1. How does the BPE tokenizer decompose the input phrase **\"Akwirw ier\"** into token IDs?\n",
    "2. What are the subwords or characters corresponding to each token ID in the tokenized output?\n",
    "3. Can the tokenizer's decoding process successfully reconstruct the original input phrase **\"Akwirw ier\"** from the token IDs? Why or why not?\n",
    "\n",
    "### Theoretical Background\n",
    "Byte Pair Encoding begins with a minimal vocabulary of single characters, such as **\"a,\" \"b,\" \"c,\"** and so on. The tokenizer builds upon this base by iteratively merging frequently co-occurring characters into subwords, and subsequently merging frequent subwords into complete words. The merging process is guided by a frequency threshold or cutoff.\n",
    "\n",
    "For example:\n",
    "- In the initial stage, the character **\"d\"** and **\"e\"** might frequently appear together in a corpus. The tokenizer merges these characters into the subword **\"de\"** if their co-occurrence exceeds the frequency cutoff.\n",
    "- This subword then becomes part of the tokenizer's vocabulary and is used to tokenize words where it occurs, such as **\"define,\" \"depend,\" \"made,\"** and **\"hidden.\"**\n",
    "\n",
    "This hierarchical merging enables the BPE tokenizer to strike a balance between granularity and generalization, efficiently encoding both common words and rare or unknown words by breaking them into smaller units.\n",
    "\n",
    "### Task Steps\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Use the `tiktoken` BPE tokenizer to process the unknown input string **\"Akwirw ier.\"**\n",
    "   - Print the token IDs generated for this input.\n",
    "\n",
    "2. **Subword Decoding**:\n",
    "   - For each token ID in the resulting list, use the tokenizer's `decode` function to convert the ID back into its corresponding subword or character.\n",
    "\n",
    "3. **Reconstruction**:\n",
    "   - Apply the `decode` method to the entire list of token IDs to reconstruct the original input string. Verify whether the reconstructed string matches the initial input, **\"Akwirw ier.\"**\n",
    "\n",
    "\n",
    "\n",
    "### Questions - Exercise 2.1\n",
    "1. What sequence of token IDs does the BPE tokenizer generate for the input **\"Akwirw ier\"?**\n",
    "2. What subwords or characters correspond to each token ID in the sequence?\n",
    "3. Does the reconstructed output from the token IDs match the original input? Explain your observations and reasoning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3rhsnoKPZFXb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5364,
     "status": "ok",
     "timestamp": 1736784530666,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "3rhsnoKPZFXb",
    "outputId": "58f9a99d-3e00-46f0-d2b0-79f73f74960c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0h7dOrP0aajo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1736784533525,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "0h7dOrP0aajo",
    "outputId": "073d36ca-4ccf-4edb-9059-7606a4944f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4Z7B-faahKB",
   "metadata": {
    "executionInfo": {
     "elapsed": 5748,
     "status": "ok",
     "timestamp": 1736784541266,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "f4Z7B-faahKB"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "P1oY-m_NalmU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1736784543091,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "P1oY-m_NalmU",
    "outputId": "6d0090e2-0c16-4354-f91f-0f1aa4d965e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Akwirw ier.\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\", \"<|unk|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pe1pTdE_bGeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1736784546491,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "pe1pTdE_bGeb",
    "outputId": "e7ca096c-c814-4008-b26c-c1d5dd72cdfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901: Ak\n",
      "86: w\n",
      "343: ir\n",
      "86: w\n",
      "220:  \n",
      "959: ier\n",
      "13: .\n"
     ]
    }
   ],
   "source": [
    "for integer in integers:\n",
    "    print(f\"{integer}: {tokenizer.decode([integer])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "FQYFtv8-a569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1736784553564,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "FQYFtv8-a569",
    "outputId": "7eedb078-a193-4fa9-a9e9-71e3a9ab34ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tzj24YX2bVBZ",
   "metadata": {
    "id": "Tzj24YX2bVBZ"
   },
   "source": [
    "1. For the input \"Akrirw ier.\", the BPE tokenizer generate the sequence \"[33901, 86, 343, 86, 220, 959, 13]\"\n",
    "\n",
    "2. For each token ID, we have this correspondance :\n",
    "33901: Ak,\n",
    "86: w,\n",
    "343: ir,\n",
    "86: w,\n",
    "220:  ,\n",
    "959: ier,\n",
    "13: .\n",
    "\n",
    "3. The reconstructed output from the token IDs match the original input. This is logicial since the BPE tokenizer only break down the original string into subword units. So if we fuse these subwork together we got back the original input. This method is used to offer flexibility and efficiency in the tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5034a-95ed-46d8-9972-589354dc9fd4",
   "metadata": {
    "id": "29e5034a-95ed-46d8-9972-589354dc9fd4"
   },
   "source": [
    "---\n",
    "\n",
    "# Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde21e07",
   "metadata": {
    "id": "fde21e07"
   },
   "source": [
    "**Exercise: Exploring Data Loader Behavior with Different Parameter Configurations**\n",
    "\n",
    "In this exercise, you will investigate how the parameters of a data loader—such as `max_length`, `stride`, and `batch_size`—affect the preparation of input-output pairs for training large language models (LLMs). By experimenting with these settings, you will gain a practical understanding of their impact on the data batching process and their implications for model training.\n",
    "\n",
    "### Objective\n",
    "You will:\n",
    "1. Observe how the data loader generates input-output pairs with different configurations of `max_length` and `stride`.\n",
    "2. Analyze how increasing the batch size changes the structure of the data and discuss the tradeoffs involved.\n",
    "3. Experiment with a batch size greater than 1 to understand how it impacts memory usage and input-output organization.\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "A data loader processes raw text data into smaller sequences suitable for training LLMs. Key parameters that influence its behavior are:\n",
    "\n",
    "1. **`max_length`**: Specifies the maximum sequence length for each input-output pair. Shorter sequences may simplify computation but can limit the context available to the model.\n",
    "   \n",
    "2. **`stride`**: Determines the step size for sliding the window over the text when creating sequences. A smaller stride increases overlap between sequences, leading to more redundancy. A larger stride reduces overlap, ensuring more unique coverage of the dataset.\n",
    "\n",
    "3. **`batch_size`**: Controls the number of sequences in a batch:\n",
    "   - **Small batches** (e.g., `batch_size=1`) are easier to process and require less memory. However, they can produce noisier gradient updates during training.\n",
    "   - **Larger batches** improve gradient stability but require more memory and computational power. This parameter is an important hyperparameter to tune during training.\n",
    "\n",
    "These parameters are central to efficient and effective preprocessing of data for training deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Task Steps\n",
    "\n",
    "1. **Experimenting with `max_length` and `stride`**:\n",
    "   - Run the data loader with two configurations:\n",
    "     - `max_length=2` and `stride=2`\n",
    "     - `max_length=8` and `stride=2`\n",
    "   - Observe the structure of the input-output pairs for each configuration and note how they differ.\n",
    "\n",
    "2. **Increasing Batch Size**:\n",
    "   - Experiment with a batch size of 8 using the following configuration:\n",
    "     ```python\n",
    "     dataloader = create_dataloader_v1(\n",
    "         raw_text, batch_size=8, max_length=4, stride=4,\n",
    "         shuffle=False\n",
    "     )\n",
    "     data_iter = iter(dataloader)\n",
    "     inputs, targets = next(data_iter)\n",
    "     print(\"Inputs:\\n\", inputs)\n",
    "     print(\"\\nTargets:\\n\", targets)\n",
    "     ```\n",
    "   - Examine the resulting `inputs` and `targets`. Consider how the data is structured when `batch_size` is increased compared to a batch size of 1.\n",
    "\n",
    "3. **Avoiding Overlap**:\n",
    "   - Analyze the effect of a `stride=4` setting. Note that this value ensures no overlap between sequences within a batch, minimizing redundancy and reducing the risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions - Exercise 2.2\n",
    "\n",
    "1. How do changes in `max_length` and `stride` affect the input-output mappings produced by the data loader?  \n",
    "2. What differences do you observe in the data when using a batch size of 8 compared to a batch size of 1?  \n",
    "3. How does using a larger stride (e.g., `stride=4`) influence the coverage of the dataset and the overlap between sequences?  \n",
    "\n",
    "---\n",
    "\n",
    "### Example Output\n",
    "\n",
    "Using the configuration:\n",
    "```python\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "```\n",
    "\n",
    "**Inputs**:\n",
    "```plaintext\n",
    "tensor([[   40,   367,  2885,  1464],\n",
    "        [ 1807,  3619,   402,   271],\n",
    "        [10899,  2138,   257,  7026],\n",
    "        [15632,   438,  2016,   257],\n",
    "        [  922,  5891,  1576,   438],\n",
    "        [  568,   340,   373,   645],\n",
    "        [ 1049,  5975,   284,   502],\n",
    "        [  284,  3285,   326,    11]])\n",
    "```\n",
    "\n",
    "**Targets**:\n",
    "```plaintext\n",
    "tensor([[  367,  2885,  1464,  1807],\n",
    "        [ 3619,   402,   271, 10899],\n",
    "        [ 2138,   257,  7026, 15632],\n",
    "        [  438,  2016,   257,   922],\n",
    "        [ 5891,  1576,   438,   568],\n",
    "        [  340,   373,   645,  1049],\n",
    "        [ 5975,   284,   502,   284],\n",
    "        [ 3285,   326,    11,   287]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Learning Outcomes\n",
    "\n",
    "By completing this exercise, you should:\n",
    "1. Understand how varying `max_length` and `stride` impacts the input-output pairs produced by the data loader.\n",
    "2. Appreciate the tradeoffs involved in choosing different batch sizes for training deep learning models.\n",
    "3. Gain insight into how stride settings can minimize redundancy and optimize dataset utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qUU0POA0c3jQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1736785323940,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "qUU0POA0c3jQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4hcbTHxre1gA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1736785486596,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "4hcbTHxre1gA",
    "outputId": "1261adbf-aec8-423c-be2c-64ecc68b3316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03Riea2xetaO",
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1736786245814,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "03Riea2xetaO"
   },
   "outputs": [],
   "source": [
    "# Importing Required Modules\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Defining the Custom Dataset Class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ma7DpmB9eNyn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1736786649429,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "ma7DpmB9eNyn",
    "outputId": "25e46d46-bb6c-4dde-c304-6facf388ec06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  40,  367],\n",
      "        [2885, 1464],\n",
      "        [1807, 3619],\n",
      "        [ 402,  271]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885],\n",
      "        [ 1464,  1807],\n",
      "        [ 3619,   402],\n",
      "        [  271, 10899]])\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=4, max_length=2, stride=2, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2DKdK6TiixOl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1736786652724,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "2DKdK6TiixOl",
    "outputId": "5d6cc2c8-4364-4a8d-afba-d44934a59078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
      "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n",
      "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
      "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
      "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257],\n",
      "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=4, max_length=8, stride=2, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7l4z84C2iual",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1736786532321,
     "user": {
      "displayName": "Nicolas CHARPENTIER",
      "userId": "18267342722895877832"
     },
     "user_tz": -60
    },
    "id": "7l4z84C2iual",
    "outputId": "eac36803-9fc2-4716-e48b-45aa7ca9a623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpXxLT1UjeZF",
   "metadata": {
    "id": "xpXxLT1UjeZF"
   },
   "source": [
    "1. Change in max_length will modify the number of token in each sequences. Change in the stride will modify the overlapping of token between sequences.\n",
    "\n",
    "  If the stride is less than the max_length there will be overlap between sequence (the same token will be seen in successive sequences), if the stride is equal to the max_length there will be no overlap (each token will be present in only one sequence) and if the stride is more than the max_length there will be space between sequence ((stride - max_length) tokens will be dropped and will not be present in the sequences).\n",
    "\n",
    "2. When we modify the batch size, we can see that the number of sequences in a batch change.\n",
    "\n",
    "3. Answered in the first question."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
