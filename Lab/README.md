For all of these Labs, only the main code was done on the exercise, no Bonus.

All the labs were done on Google Colab also, for the execution, some modification may be necessary on the path.

All the questions were answered directly on each Notebooks exercise.

Since there were done using the free version of Collab, because of the limit of RAM the runtime had to be restart between exercise, it can explain some redundancy in the code.

For the exercise 5.6, because of the RAM limit, I could only run the model GPT2_Large and not the GPT2_XL.

For the exercise 7.3, I could only train the model with the Alpaca dataset. I wasn't able to generate the json containing the answer and use llama to grade it.
