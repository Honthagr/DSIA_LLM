{"cells":[{"cell_type":"markdown","id":"51c9672d-8d0c-470d-ac2d-1271f8ec3f14","metadata":{"id":"51c9672d-8d0c-470d-ac2d-1271f8ec3f14"},"source":["# Chapter 5 - Exercises\n","\n","> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n","\n","---"]},{"cell_type":"markdown","id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e","metadata":{"id":"5fea8be3-30a1-4623-a6d7-b095c6c1092e"},"source":["# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities"]},{"cell_type":"markdown","id":"5860ba9f-2db3-4480-b96b-4be1c68981eb","metadata":{"id":"5860ba9f-2db3-4480-b96b-4be1c68981eb"},"source":["**Empirical Analysis of Token Sampling Frequencies Under Temperature Scaling**\n","\n","**Key Research Question: How does temperature-based scaling of the `softmax` probability distribution impact the sampling frequency of the specific lexical token `\"pizza\"`?**\n","\n","*Methodological Framework:*\n","Utilize the `print_sampled_tokens` function to:\n","- Empirically examine token sampling probabilities\n","- Analyze the impact of temperature scaling\n","- Quantify the sampling occurrence of the `\"pizza\"` token\n","\n","*Analytical Objectives:*\n","- Determine the precise sampling frequency of `\"pizza\"` across different temperature configurations\n","- Critically evaluate the current computational approach to sampling frequency measurement\n","- Explore potential methodological improvements for more efficient and accurate token sampling analysis\n","\n","*Key Investigative Parameters:*\n","- Primary token of interest: `\"pizza\"`\n","- Sampling method: Temperature-scaled `softmax` distribution\n","- Computational tool: `print_sampled_tokens` function\n"]},{"cell_type":"code","source":["import torch\n","\n","vocab = {\n","    \"closer\": 0,\n","    \"every\": 1,\n","    \"effort\": 2,\n","    \"forward\": 3,\n","    \"inches\": 4,\n","    \"moves\": 5,\n","    \"pizza\": 6,\n","    \"toward\": 7,\n","    \"you\": 8,\n","}\n","inverse_vocab = {v: k for k, v in vocab.items()}\n","\n","next_token_logits = torch.tensor(\n","    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",")"],"metadata":{"id":"r1MgIztYjKOv"},"id":"r1MgIztYjKOv","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def softmax_with_temperature(logits, temperature):\n","    scaled_logits = logits / temperature\n","    return torch.softmax(scaled_logits, dim=0)\n","\n","def print_sampled_tokens(probas):\n","    torch.manual_seed(123) # Manual seed for reproducibility\n","    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n","    sampled_ids = torch.bincount(torch.tensor(sample))\n","    print(int(sampled_ids[6]), \"x pizza\")"],"metadata":{"id":"lwdFk7YQj9kM"},"id":"lwdFk7YQj9kM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Temperature values\n","temperatures = [0.1, 0.5, 1, 1.5, 2, 3, 5, 10, 15, 20, 50, 100, 500, 1000]  # Original, higher confidence, and lower confidence\n","\n","# Calculate scaled probabilities\n","scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n","\n","for i, probas in enumerate(scaled_probas):\n","    print(f\"Temperature {temperatures[i]}:\")\n","    print_sampled_tokens(probas)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qGr3-n1kGhn","executionInfo":{"status":"ok","timestamp":1737206645556,"user_tz":-60,"elapsed":875,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"832dfd36-2276-4da2-ef14-f9bf9be4a4f0"},"id":"1qGr3-n1kGhn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Temperature 0.1:\n","0 x pizza\n","\n","Temperature 0.5:\n","0 x pizza\n","\n","Temperature 1:\n","0 x pizza\n","\n","Temperature 1.5:\n","2 x pizza\n","\n","Temperature 2:\n","4 x pizza\n","\n","Temperature 3:\n","15 x pizza\n","\n","Temperature 5:\n","43 x pizza\n","\n","Temperature 10:\n","62 x pizza\n","\n","Temperature 15:\n","76 x pizza\n","\n","Temperature 20:\n","85 x pizza\n","\n","Temperature 50:\n","97 x pizza\n","\n","Temperature 100:\n","99 x pizza\n","\n","Temperature 500:\n","102 x pizza\n","\n","Temperature 1000:\n","102 x pizza\n","\n"]}]},{"cell_type":"markdown","source":["We can see that the temperature impact the calcul of the softmax. If the temperature is lower than 1, the value of next_token_logits will be divided by a value lower than 1 so the disparity between high value and low value will grow. It explain that the chance of getting pizza will become really low since it's original value was already low.\n","\n","At the opposite, if the temperature is higher than 1, we will divide the next_token_logits by a value higher than 1 and as such, the disparity between high value and low value will be reduced. This will decrease the probability of high value being chosen and increase the probability of low value being chosen. For a really high temperature, all value will start to be really close to each other so the probability of each will become 1/(num_elements) and will not be impacted by the original value."],"metadata":{"id":"dJCV1ZY1lmjQ"},"id":"dJCV1ZY1lmjQ"},{"cell_type":"markdown","id":"b510ffb0-adca-4d64-8a12-38c4646fd736","metadata":{"id":"b510ffb0-adca-4d64-8a12-38c4646fd736"},"source":["# Exercise 5.2: Different temperature and top-k settings"]},{"cell_type":"markdown","id":"884990db-d1a6-4c4e-8e36-2c1e4c1e67c7","metadata":{"id":"884990db-d1a6-4c4e-8e36-2c1e4c1e67c7"},"source":["**Empirical Investigation of Generative Language Model Sampling Parameters**\n","\n","**Key Research Question: How do variations in `temperature` and `top-k` sampling parameters influence the qualitative and probabilistic characteristics of token generation in stochastic language models?**\n","\n","*Methodological Framework:*\n","Conduct a systematic empirical exploration of:\n","- Temperature scaling dynamics\n","- Top-k probability truncation mechanisms\n","- Generative output characteristics across different parameter configurations\n","\n","*Analytical Objectives:*\n","- Identify contextual applications that benefit from lower `temperature` and `top-k` settings\n","- Explore potential use cases preferring higher `temperature` and `top-k` configurations\n","- Develop nuanced understanding of sampling parameter impact on generative outputs\n","\n","*Investigative Dimensions:*\n","1. Low `temperature` and `top-k` Scenarios\n","   - Potential applications\n","   - Characteristics of generated outputs\n","   - Contextual relevance\n","\n","2. High `temperature` and `top-k` Scenarios\n","   - Potential applications\n","   - Characteristics of generated outputs\n","   - Contextual relevance\n","\n","*Recommended Experimental Protocol:*\n","1. Systematically vary `temperature` and `top-k` parameters\n","2. Meticulously document generative output characteristics\n","3. Critically analyze observed variations\n","4. Develop hypotheses about optimal parameter configurations for specific applications"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","import os\n","\n","module_path = '/content/drive/MyDrive/DSIA_LLM/lab5/main_code' # Assuming 'previous_labs.py' is in this directory\n","if module_path not in sys.path:\n","    sys.path.append(module_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVtaWd1qoyJc","executionInfo":{"status":"ok","timestamp":1737207944729,"user_tz":-60,"elapsed":22040,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"c496f305-cd7a-4c3d-afd5-e480bb19350d"},"id":"kVtaWd1qoyJc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install tiktoken\n","from gpt_generate import download_and_load_gpt2, GPTModel, load_weights_into_gpt, generate, text_to_token_ids, token_ids_to_text\n","import tiktoken\n","import torch\n","import numpy as np\n","\n","torch.manual_seed(123)\n","\n","CHOOSE_MODEL = \"gpt2-small (124M)\"\n","INPUT_PROMPT = \"Every effort moves you\"\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,     # Vocabulary size\n","    \"context_length\": 1024,  # Context length\n","    \"drop_rate\": 0.0,        # Dropout rate\n","    \"qkv_bias\": True         # Query-key-value bias\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n","\n","BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n","\n","gpt = GPTModel(BASE_CONFIG)\n","load_weights_into_gpt(gpt, params)\n","gpt.to(device)\n","gpt.eval()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qD42sK0XojT-","executionInfo":{"status":"ok","timestamp":1737208041452,"user_tz":-60,"elapsed":43627,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"604d5dc6-6bef-44c3-b3aa-e379c7f31cf3"},"id":"qD42sK0XojT-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n","Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]},{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 102kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.32MiB/s]\n","hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 140kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:20<00:00, 24.2MiB/s]\n","model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 3.42MiB/s]\n","model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.33MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.07MiB/s]\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","for i in range(5):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=5,\n","      temperature=0.1\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_9vswgPpCdF","executionInfo":{"status":"ok","timestamp":1737208288481,"user_tz":-60,"elapsed":26083,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"609da112-76ce-4823-8ffe-f89225e36292"},"id":"7_9vswgPpCdF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you forward.\n","\n","The next time you see a person who is not a good person, ask yourself, \"What is the\n","Output text:\n"," Every effort moves you forward, and you are not alone.\n","\n","The world is changing.\n","\n","The world is changing.\n","\n","The\n","Output text:\n"," Every effort moves you forward.\n","\n","The best way to do this is to take a step back and think about what you're doing.\n","\n","Output text:\n"," Every effort moves you forward, but you have to keep moving forward.\n","\n","\"I think that's what we're trying to do. We\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n"]}]},{"cell_type":"markdown","source":["We can see here that with a small temperature and small k we have some redundancy between each generation. The text still make some sense."],"metadata":{"id":"aIE3woMirU11"},"id":"aIE3woMirU11"},{"cell_type":"code","source":["torch.manual_seed(123)\n","for i in range(5):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=100,\n","      temperature=10\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKHq_Vp5rfvE","executionInfo":{"status":"ok","timestamp":1737208416223,"user_tz":-60,"elapsed":26391,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"138086ac-4b98-45d8-f8d1-ab7bb00fcc57"},"id":"HKHq_Vp5rfvE","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you toward better-trained leadership from teachers from independent academies\n","\"Each sector plays at being accountable , based both to its teachers\n","Output text:\n"," Every effort moves you so that everything must focus by itself [14 April 1209 The second wave \"rearwind\" on April 3\n","I\n","Output text:\n"," Every effort moves you by telling exactly when not I wanted! Every body movement could or doesn? Who wants fun so everyone turns heel out on Friday\n","Output text:\n"," Every effort moves you up exponentially since those first levels are tough once You find all enemies using Only Bombs/Trashing Cards again on Day 11 .\n","Output text:\n"," Every effort moves you like so too… until everyone reads 'everything in book of Revelation 1 who prayed one would eat everything 'a shepherd'. As\n"]}]},{"cell_type":"markdown","source":["With big temperature and k value, each generation is very different from each other, but we lose some sense in the generation, and the text go from one direction to another every few word."],"metadata":{"id":"yJzx5IIEsOie"},"id":"yJzx5IIEsOie"},{"cell_type":"markdown","source":["A low temperature and small k-value will lead to having close to the same generation of text over multiple execution of the programme.\n","At the opposite, a high temperature and big k-value will lead to having big difference between multiple execution of the programme.\n","\n","The first case will be used when we want a accurate and coherent answer. Can be used when we ask a question and want the answer.\n","\n","The second case will be used when we want the model to generate different sample for us, for example writing a story, proposing multiples names or title and so on."],"metadata":{"id":"URtBHC_QnOzA"},"id":"URtBHC_QnOzA"},{"cell_type":"markdown","id":"3f35425d-529d-4179-a1c4-63cb8b25b156","metadata":{"id":"3f35425d-529d-4179-a1c4-63cb8b25b156"},"source":["# Exercise 5.3: Deterministic behavior in the decoding functions"]},{"cell_type":"markdown","id":"d12229a2-1d52-46ff-b1e8-198f2e58a7d2","metadata":{"id":"d12229a2-1d52-46ff-b1e8-198f2e58a7d2"},"source":["**Deterministic Token Generation: Parametric Strategies for Eliminating Stochastic Variability**\n","\n","**Key Research Question: What specific configuration parameters within the `generate` function can systematically eliminate randomness to ensure consistently reproducible generative outputs?**\n","\n","*Methodological Framework:*\n","*Investigate comprehensive strategies to:*\n","- Suppress stochastic token generation mechanisms\n","- Enforce deterministic computational behavior\n","- Replicate the predictable output characteristics of `generate_simple`\n","\n","*Analytical Objectives:*\n","- Identify all potential parameter combinations\n","- Systematically neutralize probabilistic sampling variations\n","- Establish deterministic generative protocol\n","\n","*Critical Configuration Parameters to Examine:*\n","1. `temperature` scaling\n","2. `top_k` pruning mechanism\n","3. Random seed initialization\n","4. Sampling strategy selection\n","\n","*Recommended Experimental Protocol:*\n","1. Analyze individual parameter impacts\n","2. Identify minimal configuration requirements\n","3. Validate deterministic output generation\n","4. Compare against `generate_simple` implementation\n","\n","*Computational Implications:*\n","- Understanding stochastic suppression mechanisms\n","- Insights into generative model controllability\n","- Strategies for reproducible machine learning outputs"]},{"cell_type":"markdown","source":["We only have 2 parameters in generate that can be modify to eleminate randomness : k and temperature.\n","We can also look at the seed set to torch, to see if different seed will impact the result"],"metadata":{"id":"g_4K1HRQtduX"},"id":"g_4K1HRQtduX"},{"cell_type":"code","source":["# top k = 1000\n","# temperature = 0.0001\n","for i in range(3):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=1000,\n","      temperature=0.0001\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWbf8ofHtYpX","executionInfo":{"status":"ok","timestamp":1737209191200,"user_tz":-60,"elapsed":16425,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"5cb458c2-5280-4fbd-c2c9-f3205aae96e2"},"id":"ZWbf8ofHtYpX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n"]}]},{"cell_type":"code","source":["# top k = 1\n","# temperature = 1000\n","for i in range(3):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=1,\n","      temperature=1000\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_FI-gnluS7q","executionInfo":{"status":"ok","timestamp":1737209215422,"user_tz":-60,"elapsed":15923,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"47d61957-a4cd-4208-bf01-daa624b58e10"},"id":"M_FI-gnluS7q","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n"]}]},{"cell_type":"markdown","source":["Setting a really low k or temperature seems to be enough to generate the same things everytime. Let's look if we change the seed."],"metadata":{"id":"JWThAYDPujiD"},"id":"JWThAYDPujiD"},{"cell_type":"code","source":["torch.manual_seed(456)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FXOroRmuttg","executionInfo":{"status":"ok","timestamp":1737209084023,"user_tz":-60,"elapsed":252,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"9f436c38-a9f7-4c3d-ccc3-09a11b5792a3"},"id":"6FXOroRmuttg","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7b4c7846dc70>"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# top k = 1000\n","# temperature = 0.0001\n","for i in range(3):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=1000,\n","      temperature=0.0001\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kl-isCxfuuBa","executionInfo":{"status":"ok","timestamp":1737209108874,"user_tz":-60,"elapsed":16448,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"19220bd6-26a5-4766-ef5c-ac0c857136cd"},"id":"Kl-isCxfuuBa","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n"]}]},{"cell_type":"code","source":["# top k = 1\n","# temperature = 1000\n","for i in range(3):\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(INPUT_PROMPT, tokenizer),\n","      max_new_tokens=25,\n","      context_size=BASE_CONFIG[\"context_length\"],\n","      top_k=1,\n","      temperature=1000\n","  )\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9JsIyReuxNU","executionInfo":{"status":"ok","timestamp":1737209128056,"user_tz":-60,"elapsed":16438,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"de3b6ff8-69fa-49f8-ba59-2332031b8777"},"id":"F9JsIyReuxNU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n","Output text:\n"," Every effort moves you forward.\n","\n","The first step is to understand the importance of your work.\n","\n","The second step is to understand the\n"]}]},{"cell_type":"markdown","source":["Even after modifying the seed, we obtain the same generation. We can conclude that having a really small temperature or a unique k is enough."],"metadata":{"id":"dUp3NAJYu6qc"},"id":"dUp3NAJYu6qc"},{"cell_type":"markdown","id":"6d0480e5-fb4e-41f8-a161-7ac980d71d47","metadata":{"id":"6d0480e5-fb4e-41f8-a161-7ac980d71d47"},"source":["# Exercise 5.4: Continued pretraining"]},{"cell_type":"markdown","id":"f40044e8-a0f5-476c-99fd-489b999fd80a","metadata":{"id":"f40044e8-a0f5-476c-99fd-489b999fd80a"},"source":["**Continuation of Model Training: Stateful Resumption and Persistent Learning Dynamics**\n","\n","**Key Research Question: How can we effectively restore a machine learning model's training state across separate computational sessions, enabling seamless continuation of the pretraining process?**\n","\n","*Methodological Framework:*\n","Implement a comprehensive model and optimizer state restoration strategy involving:\n","- Weight reconstruction\n","- Optimizer state recovery\n","- Resumption of training from previously interrupted state\n","\n","*Analytical Objectives:*\n","- Demonstrate stateful model persistence\n","- Execute additional training epoch using restored model configuration\n","- Validate continuity of learning progression\n","\n","*Critical Procedural Steps:*\n","1. Load previously saved model weights\n","2. Reconstruct optimizer internal state\n","3. Reinitiate training using `train_model_simple` function\n","4. Complete one additional training epoch\n","\n","*Recommended Implementation Strategy:*\n","- Utilize precise weight and optimizer state loading mechanisms\n","- Verify complete state restoration\n","- Execute uninterrupted additional training epoch"]},{"cell_type":"code","source":["from gpt_train import create_dataloader_v1, train_model_simple, plot_losses, evaluate_model\n","import os\n","import urllib.request\n","import matplotlib.pyplot as plt"],"metadata":{"id":"zLxkmgpkyO1B"},"id":"zLxkmgpkyO1B","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt_config = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 256,  # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-key-value bias\n","}\n","\n","settings = {\n","    \"learning_rate\": 5e-4,\n","    \"num_epochs\": 3,\n","    \"batch_size\": 2,\n","    \"weight_decay\": 0.1\n","}\n","\n","###########################\n","# Initiate training\n","###########################\n","\n","torch.manual_seed(123)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","##############################\n","# Download data if necessary\n","##############################\n","\n","file_path = \"the-verdict.txt\"\n","url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/resolve/main/the-verdict.txt\" # provide a URL here or use a local file that you have already downloaded with the lab materials\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()\n","\n","##############################\n","# Initialize model\n","##############################\n","\n","model = GPTModel(gpt_config)\n","model.to(device)  # no assignment model = model.to(device) necessary for nn.Module classes\n","optimizer = torch.optim.AdamW(\n","    model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"]\n",")\n","\n","##############################\n","# Set up dataloaders\n","##############################\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","\n","train_loader = create_dataloader_v1(\n","    text_data[:split_idx],\n","    batch_size=settings[\"batch_size\"],\n","    max_length=gpt_config[\"context_length\"],\n","    stride=gpt_config[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    text_data[split_idx:],\n","    batch_size=settings[\"batch_size\"],\n","    max_length=gpt_config[\"context_length\"],\n","    stride=gpt_config[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"aQ3YsGaIxlSK"},"id":"aQ3YsGaIxlSK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################\n","# Train model\n","##############################\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=settings[\"num_epochs\"], eval_freq=5, eval_iter=1,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLS6lzQ-0pJM","executionInfo":{"status":"ok","timestamp":1737211471578,"user_tz":-60,"elapsed":389430,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"8fa0d856-79f9-432c-9471-c94dfd83c6c3"},"id":"HLS6lzQ-0pJM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 9.744, Val loss 9.840\n","Ep 1 (Step 000005): Train loss 7.845, Val loss 8.039\n","Every effort moves you,,,,,,,,,,,,,,.                                   \n","Ep 2 (Step 000010): Train loss 6.519, Val loss 6.802\n","Ep 2 (Step 000015): Train loss 6.399, Val loss 6.531\n","Every effort moves you, and, and, and, and, and, and, and, and,, and,, and, and, and, and, and, and, and,, and, and,, and, and,, and, and,\n","Ep 3 (Step 000020): Train loss 15.733, Val loss 15.784\n","Ep 3 (Step 000025): Train loss 4.937, Val loss 6.486\n","Every effort moves you.                                                 \n"]}]},{"cell_type":"code","source":["###########################\n","# After training\n","###########################\n","\n","print(f\"Training loss model 1: {train_losses[-1]:.4f}\")\n","print(f\"Validation loss model 1: {val_losses[-1]:.4f}\")\n","\n","# Save and load model\n","torch.save({\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict(),\n","    },\n","    \"model_and_optimizer.pth\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uTAotpA0pvb","executionInfo":{"status":"ok","timestamp":1737211551235,"user_tz":-60,"elapsed":47179,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"17146e5b-e502-4ed5-a689-4c85e179dc16"},"id":"_uTAotpA0pvb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss model 1: 4.9369\n","Validation loss model 1: 6.4856\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=gpt_config[\"context_length\"],\n","    top_k=1,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDdEwzrG4jDD","executionInfo":{"status":"ok","timestamp":1737211712649,"user_tz":-60,"elapsed":6736,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"5a74cd59-921e-4087-8ca1-2ebd36edf8a2"},"id":"sDdEwzrG4jDD","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]},{"cell_type":"code","source":["checkpoint = torch.load(\"model_and_optimizer.pth\")\n","\n","model2 = GPTModel(gpt_config)\n","model2.to(device)\n","model2.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","optimizer2 = torch.optim.AdamW(model2.parameters(), lr=0.0005, weight_decay=0.1)\n","optimizer2.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","\n","train_loss2, val_loss2 = evaluate_model(model2, train_loader, val_loader, device, eval_iter=1)\n","\n","print(f\"Training loss model 2: {train_loss2:.4f}\")\n","print(f\"Validation loss model 2: {val_loss2:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wzf32QJU1qeO","executionInfo":{"status":"ok","timestamp":1737211648792,"user_tz":-60,"elapsed":21092,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"dfdf39aa-7c52-46bd-a1cc-542e95554f00"},"id":"Wzf32QJU1qeO","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-40-0d6183bf716d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(\"model_and_optimizer.pth\")\n"]},{"output_type":"stream","name":"stdout","text":["Training loss model 2: 5.6775\n","Validation loss model 2: 6.4778\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate(\n","    model=model2,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=gpt_config[\"context_length\"],\n","    top_k=1,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_v27cAK4zrD","executionInfo":{"status":"ok","timestamp":1737211742268,"user_tz":-60,"elapsed":6474,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"eb49c3dd-21d1-44ae-e7b4-c9ab576ec0c8"},"id":"A_v27cAK4zrD","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]},{"cell_type":"code","source":["##############################\n","# Train model 2\n","##############################\n","\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model2, train_loader, val_loader, optimizer2, device,\n","    num_epochs=settings[\"num_epochs\"], eval_freq=5, eval_iter=1,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZPuCzwh2bF2","executionInfo":{"status":"ok","timestamp":1737212136501,"user_tz":-60,"elapsed":387258,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"55055a41-63de-4abb-99ac-042910afb769"},"id":"NZPuCzwh2bF2","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 5.124, Val loss 6.468\n","Ep 1 (Step 000005): Train loss 5.029, Val loss 6.404\n","Every effort moves you, and, and to the picture.     \"I had been, and I was, and, and was. \"I was his his of the picture.     \"I was a, and he was\n","Ep 2 (Step 000010): Train loss 4.312, Val loss 6.236\n","Ep 2 (Step 000015): Train loss 3.924, Val loss 6.205\n","Every effort moves you know a                                                \n","Ep 3 (Step 000020): Train loss 3.752, Val loss 6.163\n","Ep 3 (Step 000025): Train loss 2.628, Val loss 6.135\n","Every effort moves you know he was not that I felt--I had the fact.                                     \n"]}]},{"cell_type":"code","source":["###########################\n","# After training\n","###########################\n","\n","print(f\"Training loss model 2: {train_losses[-1]:.4f}\")\n","print(f\"Validation loss model 2: {val_losses[-1]:.4f}\")\n","\n","# Save and load model\n","torch.save({\n","    \"model_state_dict\": model2.state_dict(),\n","    \"optimizer_state_dict\": optimizer2.state_dict(),\n","    },\n","    \"model2_and_optimizer2.pth\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WkAjRa62c9h","executionInfo":{"status":"ok","timestamp":1737212285168,"user_tz":-60,"elapsed":124951,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"1fea587a-2ac1-4c84-9a38-cde4f5a0a2a0"},"id":"4WkAjRa62c9h","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss model 2: 2.6275\n","Validation loss model 2: 6.1352\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate(\n","    model=model2,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=gpt_config[\"context_length\"],\n","    top_k=1,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kY6rifwQ6PTe","executionInfo":{"status":"ok","timestamp":1737212344924,"user_tz":-60,"elapsed":4974,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"dc0e493c-1c6e-474f-d30a-6dc9011b7cde"},"id":"kY6rifwQ6PTe","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you know he was.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]},{"cell_type":"markdown","source":["For this part we have done the following:\n","\n","*   Create a new model from scrath\n","*   Train it on 3 epochs\n","* Generate a sentence\n","* Save the weight and optimizer\n","\n","Then\n","\n","* Import the weight on a empty model\n","* Generate a sentence to verify the persistence (it worked well)\n","* Verify that we Train loss and Val loss are coherent\n","* Continue the training on 3 more epochs\n","* Verify if the losses are better and the generation more coherent than before\n","\n"],"metadata":{"id":"n-NK4Mfq47LL"},"id":"n-NK4Mfq47LL"},{"cell_type":"markdown","id":"3384e788-f5a1-407c-8dd1-87959b75026d","metadata":{"id":"3384e788-f5a1-407c-8dd1-87959b75026d"},"source":["# Exercise 5.5: Training and validation set losses of the pretrained model"]},{"cell_type":"markdown","id":"7cb1140b-2027-4156-8d19-600ac849edbe","metadata":{"id":"7cb1140b-2027-4156-8d19-600ac849edbe"},"source":["**Comparative Loss Assessment: Pretrained Model Performance on Specialized Textual Domain**\n","\n","**Key Research Question: What are the comparative training and validation set losses when applying a pretrained OpenAI `GPTModel` to the \"The Verdict\" dataset?**\n","\n","*Methodological Framework:*\n","Conduct a comprehensive loss evaluation involving:\n","- Model weight initialization from pretrained OpenAI configuration\n","- Computational loss calculation across training and validation datasets\n","- Quantitative performance assessment in domain-specific context\n","\n","*Analytical Objectives:*\n","- Determine precise loss metrics for training dataset\n","- Calculate validation set loss\n","- Interpret performance characteristics of pretrained model on specialized textual domain\n","\n","*Critical Computational Procedures:*\n","1. Load pretrained OpenAI `GPTModel` weights\n","2. Prepare \"The Verdict\" dataset\n","3. Compute training set loss\n","4. Compute validation set loss\n","5. Comparative loss analysis\n","\n","*Investigative Parameters:*\n","- Model: Pretrained OpenAI `GPTModel`\n","- Dataset: \"The Verdict\"\n","- Metrics: Training and validation loss measurements\n","\n","*Recommended Analytical Approach:*\n","- Implement precise loss computation\n","- Validate computational methodology\n","- Critically interpret loss metric implications"]},{"cell_type":"markdown","source":["In the previous exercise we already load the GPT2 Small model, and we prepare the training set loss and validation loss loader. As such we only need to use the function evaluate_model"],"metadata":{"id":"xLTHxRTP8DOd"},"id":"xLTHxRTP8DOd"},{"cell_type":"code","source":["train_loss_GPT, val_loss_GPT = evaluate_model(gpt, train_loader, val_loader, device, eval_iter=10)\n","\n","print(f\"Training loss model 2: {train_loss_GPT:.4f}\")\n","print(f\"Validation loss model 2: {val_loss_GPT:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHJA3K__7Uqr","executionInfo":{"status":"ok","timestamp":1737212562998,"user_tz":-60,"elapsed":30899,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"f58c398e-7c97-406c-b454-51d7a0627cd2"},"id":"MHJA3K__7Uqr","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss model 2: 3.7547\n","Validation loss model 2: 3.5596\n"]}]},{"cell_type":"markdown","source":["If we compare to the result obtain when training the model on 10 epochs on the verdict dataset we have:\n","\n","* GPT model: Train loss 3.7547, Val loss 3.5596\n","* Small model trained: Train loss 0.252, Val loss 6.473\n","\n","We can first see that this time, it seems that the model doesn't overfit since both loss are around equal. Also the Val loss is better, so the gpt model will get better result on the validation test than the small model trained, even on the verdict dataset. However, on the training set, the small model who overfitted and learned the text will perform better than the gpt one."],"metadata":{"id":"8YwRZ-5T8U7k"},"id":"8YwRZ-5T8U7k"},{"cell_type":"code","source":["del model\n","del model2\n","del optimizer\n","del optimizer2\n","del train_losses\n","del val_losses\n","del tokens_seen\n"],"metadata":{"id":"c_kh1URt-vhb"},"id":"c_kh1URt-vhb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["del text_data\n","del token_ids"],"metadata":{"id":"oXO5cDoLAG4h"},"id":"oXO5cDoLAG4h","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3a76a1e0-9635-480a-9391-3bda7aea402d","metadata":{"id":"3a76a1e0-9635-480a-9391-3bda7aea402d"},"source":["# Exercise 5.6: Trying larger models"]},{"cell_type":"markdown","id":"b3d313f4-0038-4bc9-a340-84b3b55dc0e3","metadata":{"id":"b3d313f4-0038-4bc9-a340-84b3b55dc0e3"},"source":["**Comparative Generative Analysis: Scale and Performance Variations in GPT-2 Model Architectures**\n","\n","**Key Research Question: How do generative text characteristics vary across different GPT-2 model scales, specifically comparing the 124 million and 1,558 million parameter configurations?**\n","\n","*Methodological Framework:*\n","Conduct a systematic comparative investigation of:\n","- Generative text quality\n","- Semantic coherence\n","- Linguistic complexity\n","- Contextual understanding\n","\n","*Analytical Objectives:*\n","- Empirically assess generative performance across model scales\n","- Identify qualitative differences in text generation\n","- Explore the relationship between model parameter count and generative capabilities\n","\n","*Comparative Model Configurations:*\n","1. Smaller Model: **124 million parameters**\n","2. Larger Model: **1,558 million parameters**\n","\n","*Investigative Dimensions:*\n","- Textual coherence\n","- Semantic precision\n","- Contextual relevance\n","- Linguistic nuance\n","- Complexity of generated content\n","\n","*Experimental Protocol:*\n","1. Generate text samples using both model configurations\n","2. Conduct qualitative comparative analysis\n","3. Assess generative performance across multiple dimensions\n","4. Document observable variations in text generation characteristics\n","\n","*Recommended Analytical Approach:*\n","- Utilize consistent generation parameters\n","- Employ multiple generation trials\n","- Implement rigorous qualitative assessment\n","- Develop comprehensive comparative framework"]},{"cell_type":"markdown","source":["For this part we will first load both model, then generate multiple generation on define temperature and k value. After that we will calculate the val loss and train loss on the verdict dataset"],"metadata":{"id":"BuwnhKKp-JWH"},"id":"BuwnhKKp-JWH"},{"cell_type":"markdown","source":["We will start by the Small GPT model"],"metadata":{"id":"iOtx3Q5G9vie"},"id":"iOtx3Q5G9vie"},{"cell_type":"code","source":["torch.manual_seed(123)\n","Text = [\"The dog have\",\"Today I ate\",\"The class of Mr TAJINI is\"]\n","\n","for sentence in Text:\n","  token_ids = generate(\n","      model=gpt,\n","      idx=text_to_token_ids(sentence, tokenizer).to(device),\n","      max_new_tokens=25,\n","      context_size=gpt_config[\"context_length\"],\n","      top_k=50,\n","      temperature=1\n","  )\n","\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iM7zI1R9pbg","executionInfo":{"status":"ok","timestamp":1737213397237,"user_tz":-60,"elapsed":16763,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"c7395a49-5461-4569-8136-d95182450809"},"id":"3iM7zI1R9pbg","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," The dog have been left sitting in a parking lot while it was on its way to pick up a large dog from a neighboring street.\n","\n","Output text:\n"," Today I ate a whole bunch of vegetables. I wasn't good at eating anything on the menu so I decided to go with a vegetarian diet\n","Output text:\n"," The class of Mr TAJINI is based on the tradition of the JNU study movement. The aim of the program is to create the best possible environment to\n"]}]},{"cell_type":"code","source":["train_loss_GPT, val_loss_GPT = evaluate_model(gpt, train_loader, val_loader, device, eval_iter=10)\n","\n","print(f\"Training loss model gpt small: {train_loss_GPT:.4f}\")\n","print(f\"Validation loss model gpt small: {val_loss_GPT:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r8WB72a_Qrk","executionInfo":{"status":"ok","timestamp":1737213457683,"user_tz":-60,"elapsed":30207,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"05b806c8-b85c-4fb7-f6c8-a5e4e910fcc3"},"id":"2r8WB72a_Qrk","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss model gpt small: 3.7547\n","Validation loss model gpt small: 3.5596\n"]}]},{"cell_type":"code","source":["del gpt"],"metadata":{"id":"YurLMK1J_796"},"id":"YurLMK1J_796","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since the model is really big, I restart the kernel and import only the important element"],"metadata":{"id":"NySa3-i-DA8-"},"id":"NySa3-i-DA8-"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","import os\n","\n","module_path = '/content/drive/MyDrive/DSIA_LLM/lab5/main_code' # Assuming 'previous_labs.py' is in this directory\n","if module_path not in sys.path:\n","    sys.path.append(module_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdDqkSkrDeal","executionInfo":{"status":"ok","timestamp":1737214935012,"user_tz":-60,"elapsed":1777,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"cbcb9c2b-7fda-46ef-af54-3c1e59d3d4d9"},"id":"UdDqkSkrDeal","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from gpt_train import create_dataloader_v1, evaluate_model\n","import os\n","import urllib.request\n","import matplotlib.pyplot as plt"],"metadata":{"id":"ElvlvqoTEE-_"},"id":"ElvlvqoTEE-_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt_config = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 256,  # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-key-value bias\n","}\n","\n","settings = {\n","    \"learning_rate\": 5e-4,\n","    \"num_epochs\": 3,\n","    \"batch_size\": 2,\n","    \"weight_decay\": 0.1\n","}\n","\n","##############################\n","# Download data if necessary\n","##############################\n","\n","file_path = \"the-verdict.txt\"\n","url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/resolve/main/the-verdict.txt\" # provide a URL here or use a local file that you have already downloaded with the lab materials\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()\n","\n","##############################\n","# Set up dataloaders\n","##############################\n","\n","# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","\n","train_loader = create_dataloader_v1(\n","    text_data[:split_idx],\n","    batch_size=settings[\"batch_size\"],\n","    max_length=gpt_config[\"context_length\"],\n","    stride=gpt_config[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    text_data[split_idx:],\n","    batch_size=settings[\"batch_size\"],\n","    max_length=gpt_config[\"context_length\"],\n","    stride=gpt_config[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"1AglGoevDZLw"},"id":"1AglGoevDZLw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["del text_data"],"metadata":{"id":"FxDDQBQOEfo-"},"id":"FxDDQBQOEfo-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken\n","from gpt_generate import download_and_load_gpt2, GPTModel, load_weights_into_gpt, generate, text_to_token_ids, token_ids_to_text\n","import tiktoken\n","import torch\n","import numpy as np\n","\n","torch.manual_seed(123)\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,     # Vocabulary size\n","    \"context_length\": 1024,  # Context length\n","    \"drop_rate\": 0.0,        # Dropout rate\n","    \"qkv_bias\": True         # Query-key-value bias\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","CHOOSE_MODEL = \"gpt2-large (774M)\" # We take the large and not the XL due to RAM Limite\n","\n","model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n","\n","settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n","\n","gptXL = GPTModel(BASE_CONFIG)\n","load_weights_into_gpt(gptXL, params)\n","gptXL.to(device)\n","gptXL.eval()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISwmYxmuAYbQ","executionInfo":{"status":"ok","timestamp":1737215175013,"user_tz":-60,"elapsed":38124,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"312aff02-5e29-47b7-fcba-844f2d19f7f6"},"id":"ISwmYxmuAYbQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n","File already exists and is up-to-date: gpt2/774M/checkpoint\n","File already exists and is up-to-date: gpt2/774M/encoder.json\n","File already exists and is up-to-date: gpt2/774M/hparams.json\n","File already exists and is up-to-date: gpt2/774M/model.ckpt.data-00000-of-00001\n","File already exists and is up-to-date: gpt2/774M/model.ckpt.index\n","File already exists and is up-to-date: gpt2/774M/model.ckpt.meta\n","File already exists and is up-to-date: gpt2/774M/vocab.bpe\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","Text = [\"The dog have\",\"Today I ate\",\"The class of Mr TAJINI is\"]\n","\n","for sentence in Text:\n","  token_ids = generate(\n","      model=gptXL,\n","      idx=text_to_token_ids(sentence, tokenizer).to(device),\n","      max_new_tokens=25,\n","      context_size=gpt_config[\"context_length\"],\n","      top_k=50,\n","      temperature=1\n","  )\n","\n","  print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmrwKqSOF4aH","executionInfo":{"status":"ok","timestamp":1737215330254,"user_tz":-60,"elapsed":105990,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"da5538e3-a2ae-4f59-b35f-37c349d3a869"},"id":"vmrwKqSOF4aH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," The dog have been left a bloody mess, with many holes in their bodies and face.\n","\n","One dog had a black eye and the\n","Output text:\n"," Today I ate a whole dinner of vegetables. I wasn't hungry so I'd just eat the whole thing. One time, my mom took\n","Output text:\n"," The class of Mr TAJINI is based on the standard set by JOSE (see the chart below):\n","\n","\n","Class: 7 (12+)\n","\n","B\n"]}]},{"cell_type":"code","source":["train_loss_GPT_XL, val_loss_GPT_XL = evaluate_model(gptXL, train_loader, val_loader, device, eval_iter=10)\n","\n","print(f\"Training loss model gpt small: {train_loss_GPT_XL:.4f}\")\n","print(f\"Validation loss model gpt small: {val_loss_GPT_XL:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFhjzYsvGCcP","executionInfo":{"status":"ok","timestamp":1737215511704,"user_tz":-60,"elapsed":170386,"user":{"displayName":"Nicolas CHARPENTIER","userId":"18267342722895877832"}},"outputId":"c6d0952b-9dfd-41dc-958d-df8079f89835"},"id":"bFhjzYsvGCcP","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss model gpt small: 3.3828\n","Validation loss model gpt small: 3.2100\n"]}]},{"cell_type":"markdown","source":["When comparing the text generated, it's hard to really differenciate the model, as both generate logical sentences.\n","\n","On the other hand, when comparing the metrics on the verdict text, we can see that the large model got a slightly better score than the small one.\n","\n","To further compare the model, we should variate the temperature and k value and do several generation. Also make the model generate on mode than 25 tokens will allow to see if over the duration the sentences will still have logics.\n","\n","Due to using Google collab free version with CPU it's hard to test both as each iteration take a lot of times."],"metadata":{"id":"xtyHl4pRHZUx"},"id":"xtyHl4pRHZUx"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}